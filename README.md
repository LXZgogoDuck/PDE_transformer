# ðŸ“– PDE-Transformer: A Dynamical Systems Perspective on Information Flow in Transformers

---

## ðŸš€ Overview

This project explores the **dynamical systems view of Transformer architectures**, modeling information flow through the lens of **Partial Differential Equations (PDEs)**. By establishing an analogy between deep learning and continuous-time systems, we aim to:

- Visualize and analyze how information flows across Transformer layers.
- Develop and evaluate a PDE-inspired model that approximates this behavior.
- Provide interpretability insights grounded in **Information Bottleneck Theory** and **Neural ODE/PDE frameworks**.

---

## ðŸ“š Math formulation

> We propose modeling Transformer attention dynamics as a continuous PDE process defined by: 
> ![Equation](data/equ1.png)

Where:

![Equation](data/equation.png)

This framework bridges the gap between classical deep learning models and continuous dynamical systems, enhancing model interpretability and potentially guiding efficient model compression.

---

## ðŸ“‚ Project Structure

